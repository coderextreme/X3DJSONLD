<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">

<HTML>
<!-- SGI_COMMENT COSMOPACKAGE -->

<HEAD><!-- SGI_COMMENT COSMOCREATE --><!-- SGI_COMMENT VERSION NUMBER="1.0.1" -->
    <TITLE>An Introduction to VRML2 Animation Techniques for the 
    Production Animator</TITLE>
</HEAD>
<BODY>
<CENTER><H3 ALIGN="CENTER">
An Introduction to VRML 2.0 Animation Techniques for the Production 
Animator</H3>
</CENTER><CENTER><ADDRESS ALIGN="CENTER">
Michael Natkin</ADDRESS>
</CENTER><CENTER><ADDRESS ALIGN="CENTER">
Silicon Graphics</ADDRESS>
</CENTER><H4>
ABSTRACT</H4>
<P>
The basics of keyframe animation in VRML are described. We explore the 
aesthetic and technical considerations that an animator proficient with 
a commercial animation system must adapt to when animating for a VRML 
browser. The various sensor mechanisms that VRML provides for capturing 
user input are discussed, and simple uses of <STRONG>Script</STRONG>
 nodes are shown to convert user input into events suitable for 
triggering animation. More complex scripts can be used to make 
transitions between animation riffs for responsive character animation. 
Finally, we survey the possibilities for fully runtime-computed 
animations.</P>
<H4>
1. INTRODUCTION </H4>
<P>
As VRML worlds become ubiquitous on the World Wide Web, there will be 
great demand for professional animators to use their skills to create 
compelling content. Those who are working today with any of the 
commercial animation systems to produce film and video work will be 
able to transfer most of their expertise to the new environment. But 
there are important differences between rendered animation and 
interactive animation; new challenges to be met and new opportunities 
to create animation with custom responses to the user's input.</P>
<P>
In this paper we consider how traditional keyframe animation can be 
represented in VRML, explore the different user experience of animation 
in an interactive browser and how it affects an animator's tasks, and 
look at how to use VRML's advanced capabilities to create more dynamic, 
interactive content.</P>
<P>
We assume the reader has some familiarity with VRML syntax. Consult the 
following for more information:</P>
<UL>
    <LI>
    The VRML 2.0 Specification [VAG96] 
    <LI>
    The VRML 2.0 Handbook [HART96] 
    <LI>
    Tutorials are available at <A HREF="http://vrml.sgi.com/experts/vrml2tutorials.html">http://vrml.sgi.com/experts/vrml2tutorials.html</A>
     
</UL>
<P>
All of the example worlds mentioned in this paper in square brackets 
(e.g. [<A HREF="oneshot.wrl"><EM>oneshot.wrl</EM></A>]) can be found in 
full at this web site.</P>
<H4>
2. CANONICAL KEYFRAME ANIMATION IN VRML</H4>
<P>
Straightforward keyframe animation in VRML can be represented with 
nodes and routes such as those in Figure 1. The <EM>sensor</EM> detects 
some input in the scene, such as a click on an object. The optional <STRONG>Script</STRONG>
 node handles any logic or processing needed to determine when or how 
to start the animation. The <STRONG>TimeSensor </STRONG>starts the 
animation when it is triggered by a change to its <EM>startTime</EM>
 input, and begins producing a <EM>fraction</EM> output, which is 0.0 
at the beginning, and 1.0 when <EM>cycleInterval </EM>seconds have 
elapsed. The <EM>fraction </EM>is fanned out to a collection of 
interpolators which store the keyframes, and produce output values of 
the appropriate type depending on what type of field is to be animated 
(for example, a <STRONG>PositionInterpolator</STRONG> stores and 
outputs <EM>SFVec3f</EM> values suitable for animating the <EM>translation</EM>
 or <EM>scale </EM>fields of a <STRONG>Transform).</STRONG></P>
<P>
Conceptually, it helps to think of an animation as a triggering and 
logic unit, a single <STRONG>TimeSensor</STRONG>, and the set of 
interpolators and objects driven from that <STRONG>TimeSensor</STRONG>. </P>
<P>
<IMG SRC="sdoc.gif" ALIGN="TOP" WIDTH="592" HEIGHT="261"></P>
<CENTER><P ALIGN="CENTER">
<EM>Fig. 1 - VRML representation of keyframe animation </EM></P>
</CENTER><P>
In Figure 2, we see the simplest possible VRML keyframe animation 
following the pattern of Figure 1, without the optional <STRONG>Script</STRONG>. 
A <STRONG>Box</STRONG> is animated to travel to 3 different positions 
over a two second interval, triggered by a click on its own geometry:</P>
<HR ALIGN="LEFT">
<PRE>
    #VRML V2.0 utf8 

    DEF Cube Transform {
        translation 0 0 -5
        children [
            DEF CubeTouch TouchSensor { }
            DEF CubeTimer TimeSensor { cycleInterval 2.0 }
            DEF CubePositionInterp PositionInterpolator {
                key [0, .5, 1]
                keyValue [0 0 -5, 5 0 -5, 5 5 -5]
            }
            Shape { geometry Box { } }
        ]
    }

    ROUTE CubeTouch.touchTime TO CubeTimer.set_startTime
    ROUTE CubeTimer.fraction_changed TO CubePositionInterp.set_fraction
    ROUTE CubePositionInterp.value_changed TO Cube.set_translation 
</PRE>
<HR>
<CENTER><P ALIGN="CENTER">
<I>Fig. 2 - simplest possible keyframe animation [<A HREF="cube.wrl">cube.wrl]</A></I></P>
</CENTER><P>
Although the most common targets for animation are the fields of a <STRONG>Transform</STRONG>, 
any <EM>eventIn</EM> or <EM>exposedField</EM> in any type of node can 
be animated, e.g. the coordinates of an <STRONG>IndexedFaceSet</STRONG>, 
any of the colors of a <STRONG>Material</STRONG>, the <EM>position </EM>and <EM>orientation </EM>of 
a <STRONG>Viewpoint</STRONG> or any of the <STRONG>Light</STRONG>
 types, or even the <EM>pitch</EM> of an <STRONG>AudioClip</STRONG>.</P>
<P>
All of the VRML interpolator nodes do linear interpolation between 
keyframes. However, a clever authoring system will provide splined 
interpolation [KOCH84], and translate it to a collection of linear 
keyframes in the published VRML file.</P>
<H4>
3. HOW VRML DIFFERS FROM BROADCAST AND FEATURE ANIMATION</H4>
<H5>
ANIMATION IN THE ROUND</H5>
<P>
Animation for film and video production results in final frames where 
the animator has precise control over the viewing parameters. Animators 
often take advantage of this control to reduce the amount and precision 
of animation that must be done, or to alter an effect so it looks great 
from the known viewpoint. (This is known as &quot;cheating&quot; an 
effect). For example in a fight scene between two characters, the 
action might be staged so that the knockout punch swerves directly 
towards the camera, and the arm might be moved in a way that isn't even 
physically realistic, but looks good from that particular point of view.</P>
<P>
In contrast, when a world is viewed with a VRML browser, the user 
normally has complete freedom to roam the camera around the scene and 
view the animation from any angle. It is difficult to do a complex 
character animation that is equally artistically successful from all 
points of view. That punch which was cheated before might look very bad 
when viewed from a different angle.</P>
<P>
VRML ameliorates this problem by providing a set of <STRONG>Viewpoints</STRONG>
 that the user can select from a list in the browser, but the user may 
continue to move once they are at a preferred viewpoint, so ultimately 
the control is not in the author's hands. Often walls, floors and 
ceilings, and other obscuring objects can be used to limit the range of 
views.</P>
<P>
One other view-related detail to keep in mind: any world which will 
ever be viewed as a full page plugin, as opposed to embedded in an HTML 
page, will have its aspect ratio changed when the user resizes the 
window. <FONT COLOR="black">Notice that such resizing does not 
non-uniformly scale the view, but it may expose different amounts of 
the breadth or height of the scene than previously planned for. Use of 
a wrapper HTML page with a fixed size for the plugin will avoid this 
issue.</FONT></P>
<H5>
NO FIXED FRAME RATE</H5>
<P>
Animators have always been able to count on animating to a particular 
frame rate, generally either 24 frames per second (fps) for film, or 30 
fps for video work. Particular poses, such as the blink of an eye, the 
contact of a foot with the ground, or a flash of light, often last only 
for a single frame. </P>
<P>
VRML animation is a very different beast. There are no guarantees about 
the frame rate that a particular browser will achieve with a particular 
world on a particular platform. There is not even a guarantee that the 
frame rate will remain constant over the course of a particular 
animation. (Some authoring systems may provide the illusion of frames 
as a convenient gridding mechanism, but they will not be respected at 
playback time in a browser).</P>
<P>
The browser processes events and then renders each frame as quickly as 
it is able to. If interpolators are driven by a <STRONG>TimeSensor</STRONG>, 
they are guaranteed to complete in a fixed period of time, the <EM>cycleInterval</EM>. 
The advantage of this definition is that animation can be synchronized 
with audio or other fixed time-base media. But it puts additional 
artistic constraints on the animation. Here are some tips for working 
within those constraints:</P>
<UL>
    <LI>
    Since there is no way to control where an animation will be sampled, it 
    is imperative that critical poses last long enough to be seen in the 
    worst case. A usable world should run at a minimum of 5 (and preferably 
    10 to 15) fps on mass platforms, so one should allow at least 1/5 of a 
    second for the shortest gesture. 
    <LI>
    Keep worlds small and inexpensive to render. Not only will this help 
    keep the frame rate up and constant, it will reduce download times 
    which can be a major source of user frustration. 
    <LI>
    View animation on as wide a variety of end-user platforms as possible 
    to determine if it is acceptable; revise if necessary. Do this early in 
    the authoring cycle to avoid wasted effort. 
    <LI>
    Work with the limitations, not against them. Develop an animation style 
    that is suitable to the medium as well as the message. 
</UL>
<P>
For an animation that absolutely must be sampled at fixed points, the 
following technique is useful. Make a <STRONG>TimeSensor</STRONG> with <EM>loop</EM>
 set to <EM>TRUE</EM>; this will output a <EM>time</EM> event at every 
rendered frame. Route that to a script which simply counts frames, 
knows the total number of frames in the animation, and outputs a 
suitable fraction, stopping the <STRONG>TimeSensor</STRONG> at the end. 
Remember, this only works when the length of time it takes to play the 
animation, varying frame rates, and synchronization with other media 
are not important.</P>
<H5>
NOT A LINEAR STORY LINE</H5>
<P>
Instead of the linear story line of a film or video, interactive worlds 
are usually designed to have multiple possible paths which the user can 
follow. There may be many different animations which can be triggered 
at different times. In section 5, we discuss some of the techniques 
which can be used to compose animations which operate sequentially or 
in parallel on the same fields of an object. But the larger issue is 
that the author must be responsible for the semantic correctness of all 
possible states that a user can experience in their world; testing a 
complex VRML world is similar to testing a video game.</P>
<H4>
4. INTERACTIVE ANIMATION</H4>
<P>
VRML offers the possibility of combining keyframe animation techniques 
with interactive input. This can range from triggering a canned 
animation from a simple user input to combining, layering, and 
sequencing animations, realtime simulations, or even sophisticated 
autonomous behaviors for animated characters. All of these techniques 
require use of a <STRONG>Script</STRONG> node, which must be programmed 
in a language acceptable to the target browser. All of the examples 
below are in <STRONG>VrmlScript </STRONG>[no printed documentation is 
available as of this writing, but <A HREF="http://vrml.sgi.com/moving-worlds/spec/vrmlscript.html">http://vrml.sgi.com/moving-worlds/spec/vrmlscript.html</A>
 has the official proposal].<STRONG> </STRONG>As the behaviors become 
more complex, there may be a need for collaboration between animators 
and programmers to achieve the desired results.</P>
<H5>
NATIVE TRIGGERS</H5>
<P>
An animation can be started by routing any <EM>SFTime</EM> <EM>eventOut </EM>to 
the <EM>startTime</EM> field of the <STRONG>TimeSensor</STRONG>. The 
following native VRML node types have such an output:</P>
<UL>
    <LI>
    <STRONG>TouchSensor</STRONG> - <EM>touchTime </EM>is sent when the 
    user clicks and releases on any geometry under its parent. 
    <LI>
    <STRONG>ProximitySensor</STRONG> -<EM> enterTime</EM> and<EM>
     exitTime </EM>are sent when the user enters or exits a cuboid 
    region of space. 
    <LI>
    <STRONG>VisibilitySensor</STRONG> - <EM>enterTime</EM> is sent when 
    the user can potentially see a cuboid region of space; <EM>exitTime</EM>
     is sent when that region can no longer possibly be visible. 
    <LI>
    <STRONG>Collision</STRONG> - unlike the Sensor nodes, <STRONG>Collision</STRONG>
     is a grouping node which prevents the user from &quot;entering&quot; 
    its child geometry, and outputs a <EM>collideTime </EM>event when a 
    collision occurs. 
    <LI>
    <STRONG>Viewpoint</STRONG> - <EM>bindTime </EM>is sent when the 
    user chooses that <STRONG>Viewpoint</STRONG> from the browser's 
    list or from an <STRONG>Anchor</STRONG>. 
</UL>
<H5>
SIMPLE TRIGGER LOGIC</H5>
<P>
Although it is possible to <STRONG>ROUTE</STRONG> any of the native 
trigger events directly to a <STRONG>TimeSensor</STRONG>, it is usually 
necessary to use at least a simple <STRONG>Script</STRONG> to get the 
desired behavior. For example, consider a <STRONG>TouchSensor</STRONG>
 directly wired to start the animation of a door opening when a 
doorbell is pressed. If the user presses on the button a second time 
after the animation completes, it will jump to the beginning and play 
over again! (Nothing will happen if they click again <EM>during </EM>the 
first animation because active <STRONG>TimeSensors </STRONG>ignore <EM>startTime</EM>
 events). If this is not the intended behavior, one can use a simple 
script to make the door open only once. (This is commonly referred to 
as &quot;one-shot&quot; behavior, after similar functionality in logic 
gates). </P>
<HR>
<PRE>
    ...
    DEF DoorbellTouch TouchSensor {}
    DEF DoorOpenTimer TimeSensor { cycleTime 3 }
    DEF OneShot Script {
      eventOut       SFTime          startTime
      eventIn        SFTime          touchTime
      field          SFBool          fired       FALSE

      url  &quot;vrmlscript:
        function touchTime(value, time)
        {
           if (! fired) {
               startTime = value;
               fired     = TRUE;
           }
        }
      &quot;
    }
    ...
    ROUTE DoorbellTouch.touchTime TO OneShot.touchTime
    ROUTE OneShot.startTime TO DoorOpenTimer.set_startTime
    ROUTE DoorOpenTimer.fraction_changed TO DoorRot.set_fraction
    ROUTE DoorRot.value_changed TO Door.set_rotation
</PRE>
<HR>
<CENTER><PRE>
 <I>Fig. 3 - using a script for simple trigger logic [<A HREF="oneshot.wrl">oneshot.wrl]</A> </I>
</CENTER>
</PRE>
<P>
(For compactness, the examples will show only the most interesting 
nodes and routes in the scene; consult the online examples for full 
detail). The <STRONG>OneSho</STRONG>t script simply remembers whether 
it has already been triggered, using a boolean field named <EM>fired</EM>, 
and uses that information to decide whether to pass the <STRONG>TouchSensor</STRONG>'s <EM>touchTime</EM>
 on through to <STRONG>DoorOpenTimer</STRONG> to start the animation.</P>
<P>
If more than one <STRONG>OneShot</STRONG> is needed in a given scene, 
there must be a separate copy for each use, or the script muse be 
encapsulated in a <STRONG>PROTO</STRONG> definition. It will not work 
to <STRONG>DEF</STRONG> and <STRONG>USE</STRONG> the same <STRONG>Script</STRONG>
 node, since that instantiates the same object, just at different 
places in the scene graph. The result would be that a click on any of 
the <STRONG>TouchSensors</STRONG> would start all of the animations at 
the same time, but only once! </P>
<P>
Another common use of this script is to start an animation when the 
world is initially loaded. A <STRONG>TimeSensor</STRONG> with <EM>stopTime </EM>&lt;=<EM>
 startTime</EM> and <EM>loop</EM> set to <STRONG>TRUE</STRONG> will 
begin firing immediately; wire its output to a one-shot script to 
create a world-entry detector. For efficiency, the script should also 
disable the <STRONG>TimeSensor</STRONG> so it doesn't continue to send 
an event every frame. Here are some other possibilities for using 
simple trigger logic:</P>
<UL>
    <LI>
    Toggle between two animations, e.g. the door opens when the user clicks 
    on it the first time, and closes the the second time. 
    <LI>
    Require a sequence of user events to start an animation, e.g. the door 
    opens only after they have used a <STRONG>TouchSensor</STRONG> to 
    pick up a key in a different room, then clicked on the door. Or make 
    them tap out a melody on a piano in the correct sequence. 
    <LI>
    Only fire if other conditions in the scene are met, e.g. an elevator 
    door that only opens if the elevator is at this floor, or a &quot;You 
    Scored&quot; animation that starts if a projectile the user launched 
    ends up at the same coordinates as a moving target. 
</UL>
<H5>
A SUBTLETY OF VIEWPOINT ANIMATION</H5>
<P>
Some browsers do a short (~2 second) animation from the current <STRONG>Viewpoint</STRONG>
 to the next <STRONG>Viewpoint</STRONG> when the user selects from a 
dashboard list or <STRONG>Anchor</STRONG>. Unfortunately, the <EM>bindTime</EM>
 event is sent immediately, so if it is used to trigger an animation of 
the fields in the new <STRONG>Viewpoint</STRONG>, it will be composed 
with the automatic animation, and the result will probably not be what 
was intended. The suggested solution is to use the <EM>bindTime</EM>
 event in conjunction with a small <STRONG>ProximitySensor</STRONG> to 
determine when the user actually ends up at the position of the new <STRONG>Viewpoint</STRONG>, 
and a script to keep the sensor from triggering again until the next 
bind to the same <STRONG>Viewpoint</STRONG>. A <STRONG>PROTO </STRONG>can 
be used to encapsulate this common idiom.</P>
<H5>
USING GEOMETRIC SENSORS AS TRIGGERS</H5>
<P>
The triggers we discussed above all send out discrete events when some 
condition in the scene becomes true. But VRML also offers three 
additional sensor types which can be used to get continuous input as 
the user clicks and drags the mouse over their sibling geometry:</P>
<UL>
    <LI>
    <STRONG>PlaneSensor</STRONG> - outputs a translation in the local 
    XY plane; feels like normal 2D dragging 
    <LI>
    <STRONG>CylinderSensor</STRONG> - outputs a rotation about the 
    local Y axis with a feel like a rolling pin or turntable, depending on 
    whether the click is on the top or sides of the cylinder 
    <LI>
    <STRONG>SphereSensor</STRONG> - outputs a rotation with a feel like 
    a virtual trackball 
</UL>
<P>
These sensors cannot directly trigger a <STRONG>TimeSensor</STRONG>, 
because they do not output an <EM>SFTime</EM> event. But they can be 
used to build interactive gadgets in a scene, such as doorknobs, levers 
or sliders that the user can actually operate. The example below shows 
an excerpt from a world with a doorknob that the user can twist; the 
door opens when the handle passes a certain rotation angle, and closes 
when it goes back the other way:</P>
<HR>
<PRE>
    ...
    DEF Door Transform {
      children     [
        DEF DoorLogic Script {
          eventOut       SFTime          startClose
          eventOut       SFTime          startOpen
          eventIn        SFRotation      handleRotation
          field          SFBool          isOpen            FALSE
          url      &quot;vrmlscript:
    
              function handleRotation(value, time)
              {
                  // If the handle is rotated past 60 degrees, open door
                  if (value[3] &gt; 1.04 &amp;&amp; ! isOpen) {
                     startOpen = time;
                     isOpen = TRUE;
    
                  // If the handle is rotated back near 0 degrees, 
                  // close door
                  } else if (value[3] &lt; .1 &amp;&amp; isOpen) {
                     startClose = time;
                     isOpen = FALSE;
                  }
              }
          &quot;
        }
    
        DEF HandleGroup Transform {
          children [
           DEF DoorSensor CylinderSensor {
             minAngle      0
             maxAngle      1.57
           }
           DEF Handle Transform {
               <EM>... geometry for door handle ...</EM>
    
           }
          ]
        }
    
        <EM>... rest of door geometry ...</EM>
      ]
    }
    
    ROUTE DoorSensor.rotation_changed TO Handle.set_rotation
    ROUTE Handle.rotation_changed TO DoorLogic.handleRotation
    ROUTE DoorLogic.startOpen TO DoorOpenTimer.set_startTime
    ROUTE DoorLogic.startClose TO DoorCloseTimer.set_startTime
</PRE>
<HR>
<CENTER><PRE>
<I>Fig 4. - Script to simulate a doorknob [<A HREF="door.wrl">door.wrl]</A> </I>
</CENTER>
</PRE>
<P>
One consideration when using the<STRONG> CylinderSensor </STRONG>is 
that it maps the user's dragging into a rotation on a virtual cylinder 
centered at the local origin, and oriented along the local Y axis. The 
best way to work with it is to build the sensing geometry at the 
origin, with its intended axis of rotation aligned with Y, so that its 
local <STRONG>Transform</STRONG> (<EM>Handle</EM> in the example) is 
zeroed out, then group in the <STRONG>CylinderSensor</STRONG> and route 
from its <EM>rotation_changed</EM> output to the local <STRONG>Transform</STRONG>'s <EM>set_rotation</EM>
 field, then add a <STRONG>Transform</STRONG> (<EM>HandleGroup</EM>) 
above the whole unit to position, scale, and orient it in the scene. 
Similar considerations apply when using the other drag sensors.</P>
<P>
Here are some other ideas for ways to use drag sensors in combination 
with keyframe animation:</P>
<UL>
    <LI>
    Extend the above example to make a combination lock. A clicking noise 
    could be emitted when the tumbler gets to a correct value so the user 
    can play amateur lock-picker! 
    <LI>
    Build a slider or a dial and let the user control the speed of an 
    animation. This could be applicable for a lot of &quot;training&quot; 
    animations, where the user might want to observe in slow motion, or 
    watch more quickly, e.g. an animation of the perfect golf swing. If 
    appropriate, the same control could be used to adjust the pitch of a 
    sound for feedback. 
    <LI>
    Use the output to multiply or add to the output of interpolators, so 
    that the user can adjust the amplitude of some tracks of an animation, 
    e.g. how high a ball bounces. 
</UL>
<H5>
<FONT COLOR="black">PARAMETRIC ANIMATION</FONT></H5>
<P>
<FONT COLOR="black">For continuously looping animation, such as 
character idle cycles or background ambient animation, unvarying 
repetition can become boringly mechanical. It is often desirable to 
introduce pseudo-random variations in the speed or amplitude of these 
cycles. One way to do this is to insert a <STRONG>Script</STRONG> node 
between the <STRONG>TimeSensor</STRONG> and the interpolator(s). This 
script can alter the <EM>fraction</EM> by adding a random component, or 
synthesize the <EM>fraction</EM> from scratch with a mathematical 
function of time. With clever programming, many effects such as pauses 
in the cycle or chaotic behavior can be achieved.</FONT></P>
<P>
<FONT COLOR="black">For example, a frog character might be expected to 
breathe at somewhat irregular intervals, and to expand its chest more 
fully on some breaths than others. A <STRONG>CoordinateInterpolator</STRONG>
 with the exhale keyframe at <EM>fraction</EM> = 0.0 and a maximally 
expanded chest at <EM>fraction</EM> = 1.0, driven by an out-of-phase 
sum of sine and cosine waves, and clamped to the range 0-1 will work 
well. The frog will pause nicely between breaths while the waveform 
travels below 0.</FONT></P>
<H4>
5. COMPOSABLE ANIMATION</H4>
<P>
For character or avatar animation, a common technique is to have a 
number of canned animations (&quot;riffs&quot;) to express different 
moods or actions. Lets assume we already have a script that can 
determine which animation or animations should be active, based on 
whatever collection of state information is available. The next 
question is how to make smooth transitions between riffs, and possibly 
how to combine multiple riffs at the same time. A range of strategies 
are available, discussed below in order of implementation complexity. 
For simplicity, we will assume the example of a robot with the 
following riffs: Walk, Run, Kick, Nod, and LookLeft. Each of the 
gestures returns whatever <STRONG>Transforms</STRONG> it controls back 
to the rest position at the end of each cycle. Walk and Run are really 
in place motions; some other animation or script is assumed to be 
controlling the robot's overall position and orientation in the world.</P>
<H5>
SCRIPT-FREE SOLUTIONS</H5>
<P>
The simplest case is when the animations aren't really for the same 
node. Although both Walk and Nod affect parts of the robot hierarchy, 
they are really completely different <STRONG>Transform</STRONG>s that 
can be run either in sequence or even at the same time, and no 
additional logic is needed to reconcile them. (Assuming the robot 
doesn't bend at the neck while walking). </P>
<P>
Next consider Nod and LookLeft. They both affect the neck joint, so if 
they are to be played at the same time, something must be done to add 
them together. The easiest solution is to simply add an extra <STRONG>Transform</STRONG>
 layer at the next level. The parent <STRONG>Transform</STRONG> will be 
animated for LookLeft, and the child <STRONG>Transform</STRONG> for 
Nod. That way, Nod will operate about the correct local axis, even if 
the head is turned. </P>
<H5>
JUMP CUTS</H5>
<P>
Now consider Walk, Run, and Kick. They certainly all affect the leg 
joints, and adding them together will not produce the desired effect: 
the robot can't kick while in the middle of a dead run. A simple, but 
low quality solution is to simply do a jump cut. Send a <EM>stopTime</EM>
 event to the current animation, and a <EM>startTime</EM> to the next, 
and live with the discontinuities. This may be acceptable in some 
situations. ([<A HREF="robot.wrl"><EM>robot.wrl</EM></A>] demonstrates 
this and the the previous two techniques.)</P>
<H5>
STATE MACHINES</H5>
<P>
A better effect can be achieved by having the script remember what the 
desired new state is, wait until the current animation finishes a cycle 
and returns to the rest position, and then performing the jump cut. The <EM>cycleTime</EM>
 output from the <STRONG>TimeSensor</STRONG> can be used to determine 
when to make the switch. The discontinuity is avoided, but the reaction 
is delayed until the cycle ends; when the cycle is short, this may work 
quite well. This solution is often used in video fighting games. (See [<A HREF="state.wrl"><EM>state.wrl</EM></A>]).</P>
<H5>
INTERPOLATION BETWEEN ANIMATIONS</H5>
<P>
Better still, but much more difficult to implement, is transitional 
interpolation. Assume the animation is part way through a Walk cycle, 
and the script wants to switch to Run. The script starts Run using the 
same <EM>startTime</EM> as it did for Walk, and uses the rest of the 
cycle to interpolate from 100% of the Walk interpolator output values 
and 0% of Run, to 100% of Run and 0% of Walk. At the end of the cycle, 
it switches completely to Run. The reason this is painful to implement 
for multi-jointed animations is that the script must intercept all of 
the interpolator outputs from each animation, take a weighted average 
of them, and route the results to the destination fields. See [<A HREF="interp.wrl">interp.wrl</A>]. Note that this technique 
might not be very appropriate for a transition from Walk to Kick (for 
example), because the Kick may only be visually correct if it is played 
starting fully from the rest position. </P>
<P>
Building on the previous idea, one could also do a continuous 
interpolation between different animations. Instead of switching 
between Walk and Run strides at a fixed threshold, the script could 
continuously interpolate between the two based on velocity. For example 
it could do 100% Walk when moving at or below 5 meters/second, 100% Run 
when moving over 15 meters/second, and a smooth combination of the two 
at intermediate speeds. See [<A HREF="cinterp.wrl">cinterp.wrl</A>]. <FONT COLOR="black">The goal of such an approach is to integrate artistic, 
expressively posed animations within the parametric performance of 
programmed play. </FONT></P>
<H4>
6. MORE COMPLEX BEHAVIORS</H4>
<P>
In addition to keyframe animation, it is possible to do entirely 
runtime computed animation in VRML. With a sufficiently clever script, 
the only limitations are imagination, programming ability, and most 
importantly, execution time. The list below is only a starting point. </P>
<UL>
    <LI>
    Constraint systems can be used to enforce position or velocity 
    relationships between objects. [BARA96, BARZ88] 
    <LI>
    Simple forward dynamics can be used to simulate other phenomena, such 
    as falling bodies or accelerating vehicles [POTT73]. VRML doesn't 
    provide any mechanism for detecting object-to-object collisions, so to 
    do a ball bouncing off of a wall, the script needs to do its own 
    explicit <STRONG>Transform</STRONG> comparisons. 
    <LI>
    Spring-mass systems for simulation of many kinds of physical phenomena, 
    such as chains and flags. [WEIL86] 
    <LI>
    Inverse dynamics [TERZ87] is a technique where physical forces are 
    computed based on a goal state, and then applied to the objects at each 
    frame. There is a simple example in [<A HREF="damp.wrl"><EM>damp.wrl</EM></A>]. 
    showing how damped acceleration can be used allow one object to follow 
    another without changing speed or direction too quickly. 
    <LI>
    A full inverse kinematic solution for an arbitrary figure would 
    probably be too expensive to compute every frame, but a simple 
    two-joint robot arm is feasible. [KUDE92], [ZHAO89], [PHIL91] 
    <LI>
    Flocking. [REYN87], [ROBE93] demonstrated that a collection of objects 
    following simple rules based on each other's positions can exhibit 
    surprisingly interesting emergent behaviors. 
    <LI>
    Particle systems techniques [REEV83] can be used for effects such as 
    fountains, fireworks, and confetti, and are relatively inexpensive to 
    compute. 
    <LI>
    Eventually we expect to see the emergence of sophisticated, autonomous 
    agents in the world, guided by artificial intelligence techniques. 
    [HAUM88] 
</UL>
<H4>
7. CONCLUSION</H4>
<P>
VRML 2.0 is in its infancy as a standard on the web. As it matures, 
many new animation styles and techniques will be developed. Because 
VRML supports a wide range of media, and sophisticated scripting and 
interaction, there is the potential for unprecedented collaboration 
between graphic and user interface designers, directors, modelers, 
animators and programmers. We hope the ideas in the paper will be a 
useful starting point for people who have been working in traditional 
production, and we look forward to seeing amazing new worlds.</P>
<H4>
ACKNOWLEDGEMENTS</H4>
<P>
The author would like to thank Rob Myers, who contributed whole topics 
to this paper, Paul Strauss and Rich Gossweiler for the helpful 
discussions, David Story, Rick Pasetto and Daniel Woods for the 
critical reading, and the entire Starfish / Cosmo group at Silicon 
Graphics for developing great VRML tools.</P>
<H4>
REFERENCES</H4>
<P>
[BARA96] Baraff, David, &quot;Linear-Time Dynamics using Lagrange 
Multipliers&quot;, SIGGRAPH '96, pp. 137-146.</P>
<P>
[BARZ88] Barzel, Ronen and Barr, A. H., &quot;A Modeling System Based 
on Dynamic Constraints&quot;, SIGGRAPH '88, pp. 179-188.</P>
<P>
[HART96], Hartman, Jed and Wernecke, Josie, <EM>The VRML 2.0 Handbook</EM>, 
Addison-Wesley, Reading, MA, 1996 , 412 pp. </P>
<P>
[HAUM88] Haumann, D. and Parent, R., &quot;The Behavioral Test-Bed: 
Obtaining Complex Behavior from Simple Rules&quot;, <EM>The Visual 
Computer</EM>, Vol. 4, No.6, 1988, pp. 332-347.</P>
<P>
[KOCH84] Kochanek, D and Bartels, R.,&quot;Interpolating Splines with 
Local Tension, Continuity and Bias Control,&quot; SIGGRAPH '84, pp. 
33-42.</P>
<P>
[KUDE92] Kuder, Karen, &quot;Using Inverse Kinematics to Position 
Articulated Figures&quot;, <EM>Proceedings of the 1992 Western Computer 
Graphics Symposium</EM>, p. 121.</P>
<P>
[PHIL91] Phillips, Cary and Badler, Norman I., &quot;Interactive 
Behaviors for Bipedal Articulated Figures&quot;, SIGGRAPH '91, pp. 
359-362.</P>
<P>
[POTT73] Potter, David,<EM> Computational Physics</EM>, John Wiley 
&amp; Sons, New York, 1973.</P>
<P>
[REEV83] Reeves, W., &quot;Particle Systems - A Technique for Modeling 
a Class of Fuzzy Objects&quot;, SIGGRAPH '83, pp. 359-376.</P>
<P>
[REYN87] Reynolds, C., &quot;Flocks, Herds, and Schools: A Distributed 
Behavioral Model,&quot; SIGGRAPH '87, pp. 25-34. </P>
<P>
[ROBE93] Robertson, Barbara, &quot;Powerful Particles,&quot; Computer 
Graphics World, Vol. 16, No. 7, July 1993, pp. 40-48 </P>
<P>
[TERZ87] Terzopoulos, D., Platt, Isaacs, P. and Cohen, M., 
&quot;Controlling Dynamic Simulation with Kinematic Constraints, 
Behavior Functions and Inverse Dynamics&quot;, SIGGRAPH '87, pp. 
215-224. </P>
<P>
[WEIL86] Weil, Jerry, &quot;The Synthesis of Cloth Objects&quot;, 
SIGGRAPH '86, pp. 49-54.</P>
<P>
[VAG96], Vrml Architecture Group, &quot;VRML 2.0, The Virtual Reality 
Modeling Language Specification, ISO/IEC CD 14772&quot;, available at <A HREF="http://vag.vrml.org/VRML2.0/FINAL/">http://vag.vrml.org/VRML2.0/FINAL/.</A></P>
<P>
[ZHAO89] Zhao, Jianmin and Badler, Norman I., &quot;Real Time Inverse 
Kinematics with Joint Limits and Spatial Constraints&quot;, Technical 
Report MS-CIS-89-09, Dept. of Computer and Information Science, 
Universe of Pennsylvania, Philadelphia, PA, 1989.</P>
</BODY>
</HTML>
